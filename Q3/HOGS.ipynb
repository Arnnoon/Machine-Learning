{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8ddf4ed"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this was created because Google Colab does not allow cv2.imshow, so must patch by cv2_imshow.  \n",
        "#If we switch over to regular jupyter notebook not on Colab, we can change c2_imshow to cv2.imshow.  \n",
        "from google.colab.patches import cv2_imshow #only used when running in Google Colab \n",
        "def my_imshow(title, img ):\n",
        "  print(title)\n",
        "  cv2_imshow(img) #should be changed to c2.imshow when not in Colab"
      ],
      "metadata": {
        "id": "LRVdvcUnzoOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c7c5679",
        "outputId": "580df81c-8f4b-44f4-f2a1-765315c931ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image file read success...\n"
          ]
        }
      ],
      "source": [
        "path = \"\"\n",
        "fileName = path + \"digits.png\" \n",
        "\n",
        "#RGB images in BGR order in OpenCV\n",
        "image = cv2.imread(fileName, cv2.IMREAD_COLOR)\n",
        "\n",
        "# Print error message if image is null\n",
        "if image is None:\n",
        "    print('Could not read image')\n",
        "else: \n",
        "    print(\"Image file read success...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cut out each digit as a labeled dataset and rescale all images from 20 x 20 to 24 x 24.\n",
        "sub_image = []\n",
        "desired_dimension = (24, 24)\n",
        "height, width = image.shape[:2]\n",
        "print(height, width)\n",
        "for i in range(height // 20):\n",
        "    sub_image.append([])\n",
        "    for j in range(width // 20):\n",
        "        sub_image[-1].append(cv2.resize(image[i*20:(i+1)*20, j*20:(j+1)*20], desired_dimension))\n",
        "\n",
        "# Convert sub_image to a numpy array\n",
        "sub_image = np.array(sub_image)\n",
        "\n",
        "# Create a 50x100 array and initialize it to -1\n",
        "label = np.full((50, 100), -1)\n",
        "\n",
        "# Fill the array with alternating values of 0-9\n",
        "for i in range(10):\n",
        "    label[i*5:(i+1)*5, :] = i\n",
        "\n",
        "# Split sub_image into training and testing sets\n",
        "sub_train, sub_test, train_label, test_label = train_test_split(sub_image, label, test_size=0.2, random_state=42)\n",
        "flat_train_label = train_label.flatten()\n",
        "flat_test_label = test_label.flatten()\n",
        "print(flat_train_label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhPxLibEC6ZF",
        "outputId": "ea6e51a6-4acc-47f8-ea35-20da5a130c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 2000\n",
            "(4000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X_train and X_test to float and scale them\n",
        "float_train = np.float32(sub_train) / 255.0\n",
        "float_test = np.float32(sub_test) / 255.0\n",
        "\n",
        "# Calculate the gradients for the training and testing sets\n",
        "Gx_train = []\n",
        "Gy_train = []\n",
        "\n",
        "for row in float_train:\n",
        "    for img in row:\n",
        "        Gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=1)\n",
        "        Gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=1)\n",
        "        Gx_train.append(Gx)\n",
        "        Gy_train.append(Gy)\n",
        "\n",
        "Gx_train = np.array(Gx_train)\n",
        "Gy_train = np.array(Gy_train)\n",
        "\n",
        "print(Gx_train.shape, Gy_train.shape)"
      ],
      "metadata": {
        "id": "ja_2axax6gZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fa0be6-80c3-4d3c-9a87-38feffc30d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4000, 24, 24, 3) (4000, 24, 24, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate gradient magnitude and direction ( in degrees )\n",
        "magnitude, angle_degrees = cv2.cartToPolar(Gx_train, Gy_train, angleInDegrees=True)\n",
        "print(magnitude.shape, angle_degrees.shape)\n",
        "# You can then do the histogram yourself ... it might be easier."
      ],
      "metadata": {
        "id": "4TRl3vEb6Thm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a603807c-ce80-460e-e7fc-33e41306b62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4000, 24, 24, 3) (4000, 24, 24, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "HOG using OpenCV. \n",
        "\n",
        "win_size – Detection window size. Align to block size and block stride.\n",
        "block_size – Block size in pixels. Align to cell size. Only (16,16) is supported for now.\n",
        "block_stride – Block stride. It must be a multiple of cell size.\n",
        "cell_size – Cell size. Only (8, 8) is supported for now.\n",
        "nbins – Number of bins. Only 9 bins per cell are supported for now.\n",
        "win_sigma – Gaussian smoothing window parameter.\n",
        "threshold_L2hys – L2-Hys normalization method shrinkage.\n",
        "gamma_correction – Flag to specify whether the gamma correction preprocessing is required or not.\n",
        "nlevels – Maximum number of detection window increases.\n",
        "'''\n",
        "winSize = desired_dimension\n",
        "blockSize = (16,16) #OpenCV only supports 16 x 16 block sizes \n",
        "blockStride = (8,8) #multiple of cell size. Here it is multiple of 1.  \n",
        "cellSize = (8,8) #OpenCV only supports 8x8 cell size. That means each Block will have 4 histograms\n",
        "nbins = 9 #OpenCV only supports 9 orientations per cell.  That means 1 block has 4 x 9 = 36 features\n",
        "derivAperture = 1\n",
        "winSigma = 4.\n",
        "histogramNormType = 0\n",
        "L2HysThreshold = 2.0000000000000001e-01\n",
        "gammaCorrection = 0\n",
        "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,\n",
        "                        histogramNormType,L2HysThreshold,gammaCorrection)\n",
        "#compute(img[, winStride[, padding[, locations]]]) -> descriptors\n",
        "winStride = (8,8)\n",
        "padding = (8,8)\n",
        "locations = ((0,0),) #we run it at only one location of image (entire image).  Can be used to run at sub-image parts. \n",
        "\n",
        "train_hog_feature = np.zeros((len(sub_train), len(sub_train[0]), 4*4*nbins))\n",
        "#print(hog_feature.shape)\n",
        "for i in range(len(sub_train)):\n",
        "    for j in range(len(sub_train[i])):\n",
        "        feature_vector = hog.compute(sub_train[i][j],winStride,padding,locations)\n",
        "        feature_vector = feature_vector.reshape(-1) # Flatten the 3D array to 1D array\n",
        "        train_hog_feature[i][j] = feature_vector\n",
        "\n",
        "train_hog_feature = train_hog_feature.reshape((train_hog_feature.shape[0]*train_hog_feature.shape[1], train_hog_feature.shape[2]))\n",
        "#print(\"HOG feature: \\n\", train_hog_feature)\n",
        "number_of_train_features = train_hog_feature.shape\n",
        "# There are 7 horizontal and 15 vertical blue windows, making a total of 7 x 15 = 105 positions.\n",
        "# Each 16×16 block is represented by 4 of 8 x 8 blocks. Each block is 9 histogram values. \n",
        "# Each 16x16 block has 4 x 9 = 36×1 feature vector. \n",
        "# The feature is a concatenation of 105 such features to get \n",
        "# a vector of dimension 3,780 from 105 x 36.  \n",
        "\n",
        "print(\"Number of train Features: \", number_of_train_features)\n",
        "# print(\"This is from 9 histo values per cell x 4 cells per 16x16 block x (7x15) blocks per image: \", 9*4*7*15)\n",
        "\n",
        "test_hog_feature = np.zeros((len(sub_test), len(sub_test[0]), 4*4*nbins))\n",
        "# print(hog_feature.shape)\n",
        "for i in range(len(sub_test)):\n",
        "    for j in range(len(sub_test[i])):\n",
        "        feature_vector = hog.compute(sub_test[i][j],winStride,padding,locations)\n",
        "        feature_vector = feature_vector.reshape(-1) # Flatten the 3D array to 1D array\n",
        "        test_hog_feature[i][j] = feature_vector\n",
        "\n",
        "test_hog_feature = test_hog_feature.reshape((test_hog_feature.shape[0]*test_hog_feature.shape[1], test_hog_feature.shape[2]))\n",
        "# print(\"HOG feature: \\n\", test_hog_feature)\n",
        "number_of_test_features = test_hog_feature.shape\n",
        "print(\"Number of test Features: \", number_of_test_features)"
      ],
      "metadata": {
        "id": "xuyp17fD6xRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c8f3db-612d-45f3-8e63-86d56450405f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train Features:  (4000, 144)\n",
            "Number of test Features:  (1000, 144)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sub_train images to grayscale and flatten them\n",
        "sub_train_gray = []\n",
        "for row in sub_train:\n",
        "    for img in row:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        gray_img_flat = gray_img.flatten()\n",
        "        sub_train_gray.append(gray_img_flat)\n",
        "\n",
        "# Convert sub_test images to grayscale and flatten them\n",
        "sub_test_gray = []\n",
        "for row in sub_test:\n",
        "    for img in row:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        gray_img_flat = gray_img.flatten()\n",
        "        sub_test_gray.append(gray_img_flat)"
      ],
      "metadata": {
        "id": "01KiujnhMdX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_classifier.fit(train_hog_feature, flat_train_label)\n",
        "HOG_K5 = knn_classifier.predict(test_hog_feature)\n",
        "\n",
        "accuracy = sum(HOG_K5 == flat_test_label) / len(flat_test_label)\n",
        "print(\"HOG_K5 Accuracy:\", accuracy)\n",
        "\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "knn_classifier.fit(train_hog_feature, flat_train_label)\n",
        "HOG_K1 = knn_classifier.predict(test_hog_feature)\n",
        "\n",
        "accuracy = sum(HOG_K1 == flat_test_label) / len(flat_test_label)\n",
        "print(\"HOG_K1 Accuracy:\", accuracy)\n",
        "\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_classifier.fit(sub_train_gray, flat_train_label)\n",
        "GRAY_K5 = knn_classifier.predict(sub_test_gray)\n",
        "\n",
        "accuracy = sum(GRAY_K5 == flat_test_label) / len(flat_test_label)\n",
        "print(\"GRAY_K5 Accuracy:\", accuracy)\n",
        "\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "knn_classifier.fit(sub_train_gray, flat_train_label)\n",
        "GRAY_K1 = knn_classifier.predict(sub_test_gray)\n",
        "\n",
        "accuracy = sum(GRAY_K5 == flat_test_label) / len(flat_test_label)\n",
        "print(\"GRAY_K1 Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTj3fkDC4eGq",
        "outputId": "037d2c3c-e7b5-479d-8c55-cac041ac41b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOG_K5 Accuracy: 0.956\n",
            "HOG_K1 Accuracy: 0.949\n",
            "GRAY_K5 Accuracy: 0.924\n",
            "GRAY_K1 Accuracy: 0.924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desired_dimension = (24, 24)\n",
        "row_start = ((random.randint(0, 8)*5) + 3) * 20\n",
        "col_start = random.randint(0, 94) * 20\n",
        "cut_image = image[row_start:row_start+(20*4), col_start:col_start+(20*4)]\n",
        "\n",
        "sub_cut = []\n",
        "desired_dimension = (24, 24)\n",
        "height, width = cut_image.shape[:2]\n",
        "for i in range(height // 20):\n",
        "    sub_cut.append([])\n",
        "    for j in range(width // 20):\n",
        "        sub_cut[-1].append(cv2.resize(cut_image[i*20:(i+1)*20, j*20:(j+1)*20], desired_dimension))\n",
        "\n",
        "first_label = int(((row_start/20)-3)/5)\n",
        "ran_col = random.randint(0, 3)\n",
        "test_cut = []\n",
        "train_cut = []\n",
        "test_cut_label = [first_label, first_label, first_label+1, first_label+1]\n",
        "train_cut_label = [first_label, first_label, first_label, first_label, first_label, first_label, first_label+1, first_label+1, first_label+1, first_label+1, first_label+1, first_label+1]\n",
        "for row in range(len(sub_cut)):\n",
        "    for col in range(len(sub_cut[row])):\n",
        "        if col == ran_col:\n",
        "            test_cut.append(sub_cut[row][col])\n",
        "            cv2.rectangle(cut_image, (ran_col*20, 0), ((ran_col*20)+20, 80), (255, 255, 255), 2)\n",
        "        else:\n",
        "            train_cut.append(sub_cut[row][col])\n",
        "\n",
        "# Display the image with bounding box\n",
        "my_imshow('Selected Sub-Image with Bounding Box', cut_image)\n",
        "for img in test_cut:\n",
        "    my_imshow('Test-Image', img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "fbyPkOAREDiE",
        "outputId": "1dc794b2-18bd-441e-d408-ed8b9b254403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Sub-Image with Bounding Box\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=80x80 at 0x7EFE0F2FA7C0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=24x24 at 0x7EFE0F2AF670>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAIAAABvFaqvAAAC10lEQVR4nN2UQU/qQBDHaXdFrCK1pLWCiCZSlWgwXFAxabyoUQ/eOOrZj+PRD6EmJoqctGn0QMJJpUZt2kRoSETAdBdNoH0HEqMFnh5f3v82yc5vZ+a/Oy7X/yMAAEEQnyHxl6MdBSEcGhoKhUJ+vx9jXCwWS6USxvi3IJIk+/v7aZoeGRmJRqOLi4vhcLhcLsuynMlknp6e4G8QAACapmOx2PLy8tLS0tzc3ODgoGVZCCGSJO/u7n4GAQB8Pp8gCDs7OysrK4FAACF0f39vGIaiKNfX1zc3N6+vry6X628gjuMSicTGxkY0Go1EIgMDA/l8/vj4+Pz8HGOMEKpWqxhjy7Jc3YYNIYzH46urq6IoTk1N+f3+SqUiy/Lp6aksy5qmtZK/pThigiB6enpmZ2dTqdT6+jpFUblcrre3V1GUdDqdzWZbjXS4ux3k9Xq3trY2NzdZlpUkaX9/nyAIXdcNw8AYd5tDZ1AsFuN5vlwu67pumqaqqvV6/ePjoxulgwAA4+Pjh4eHtVqt0Wi8vb2pqnpycrK3tycIAgDgtxXZtm2a5uXlZa1WC4VCwWDQ6/Umk0lBEOLx+NHR0cXFBUKoHeR0jSAIt9s9NjbGMAzDMBzH0TQ9PT0tiqLH48lkMgcHB7lc7oeKWpZBCFVVfXh4gBC63W6PxzM6OooQ2t7eXlhYuL29VRSlXq/btv01l/waUBQ1MzMjiuL8/Hw4HGYYhqIoy7IeHx8lSdJ1PRgMJpNJlmVJ8luis6Lh4eHd3d21tTXDMLLZbMss0zRLpRLP862uA4EAy7LFYrHZbHYFmaaZz+dTqdTExEQikWg2m7ZtW5b1/v7e19dHUdTz8/PV1ZWmaQ6KE1SpVM7OzgAAkUhkcnKS4zgIoW3btm0DAAqFgiRJ6XS6Wq22fxGnayRJ8jzf8svn80EIP314eXnRNK1QKHR8mV0XGwDAsUwbjUZ7R/+w/gDm9EkDvGUZjQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=24x24 at 0x7EFE0F2AFD90>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAIAAABvFaqvAAACzUlEQVR4nM2Uz07qQBTGp1OmaQUt1TZKkxoXJgR1QVy5whAj3Ro1voPxLXwMN+xduZWViUZ2rdEa4oZqjaQFlP4hrTi2vQsSLypwvTu/7cz5zTnfOXMA+G0ixp0RBEmSCCEAAEVRCCGMse/7GOOfgiCEPM9LkpTJZFiWJQhCEIRkMum6rq7rhmH4vt9oNHzfj+O4H5IYChIEoVgsFovFbDabSqV6vZ4gCBRFmaapqqqiKJZleZ73+voahuFIEEJoc3Nzf39/dXXVcZzHx8dGo6FpmmVZlmU1m03LsjqdzmA6Q4QQWltbOz8/b7VaJycnW1tbs7Oz09PTU1NTqVRqYmKCpmmKohKJBEF8suVrRhBCQRA4jjs7OyuXy5eXl47jAADGPT6qtCiKAACtVsswDNu2x8ePBIVhqOu6qqozMzOlUollWV3XLcv6MHWUhrSfJMlSqbSzsyNJkq7riqJUq9WHh4dut9tP9qegPmtxcbFQKGxsbORyuevr63K5rCiK67qjWCMnO5FIIIQymYwsy3t7e47jHB8fVyqVdrv9f6C+EEI8z2ez2UKhIIri/f19pVJRVfV7E4dP9ocwxqZp2rbdbrdlWc7n8zzPY4xrtdr7+/vgTTgeBACI4zgIAk3TTk9Pb29vFxYWdnd3k8nkuIGkaVoQBIZhOp2O53lvb2+D1mqaFoYhTdOyLB8dHXmeN1jgX1D/i29vb+dyuWq1enNzY5pmf2lEURTHMUEQ3W63Xq8HQUCS5MiM4jh+fn6+uLjI5/MHBwfpdPrl5aVWq93d3T09PUVRRNP00tLS+vo6y7Lfzf5EhRAyDDM/P7+ysrK8vCyK4uTkJM/zoiiSJAkhxBg3m82rq6vDw0PbtgdxQ9pPEATHcTzPcxyXTqfn5uYkSSJJEgDguq5hGPV6XdO0L3vyH3MEIaRpmmEYCCEAAGMcBEGv1xsf9Tv0B42/ZXLebwkfAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=24x24 at 0x7EFE0F2AFA90>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAIAAABvFaqvAAAC8ElEQVR4nO3UTUvjQBgH8GQm1dTEVAIpaIu1aYUipbEVAp6EIsUcfCFqBfFjiH4WD3oRhKooIhpRiQcRK7UgiFFEkFpb0aRp1dLSKskedtFdV1nd0x72OQ7/+fHMDM8gyL9W6OejOI7TNE2SZD6fz+VyhmH8DWS324PBYCgUIkkykUhIklSpVH4OYH8kAAA0TYfD4ZGRkVAopGlaLpeDEH7+KAiCICiK1tfXj42NxePxQqFweHg4OTnpdDq/DFEUFYlEFEUpl8ubm5vRaJQgiC8rNptNEIStra1SqXR1dfW9l4/CH96Ry+UKh8OiKAaDwWQyub6+LknS7e3t1yCGYSKRyOjoaCAQuLy8nJ6elmX58fHR4XDYbDYAAABAVVVN00ql0oeQxWLheV4URZ7nM5nM4uLiwsIChLC9vZ3nebfbjWEYhFBRlIODg9PTU13X3+sQw5qbm6emptLptKqqsVjM6/USBNHd3R2LxVRVLRaLDw8PlUoln8/Pz8/39PS80xGEkGGY8fFxQRAAAKurqzMzMzc3N4IgTExMeL3ei4uL3d3du7u7vr4+v99vt9spinoLYRjmcrmi0Whvby9BEEtLS7Ozs2dnZ263e3h4mGXZeDy+srKiqmpXV5fH47m+vt7Y2Egmk28hiqI4jhscHGxqapJleW1t7fj4mKbpoaGhzs5OgiBwHA8EAjRN+3y+dDq9vLwsSVImk3kLWa1Wh8PR2toKIdQ0raamhuM4juNEUWxsbEQQxOPxUBRVLpfPz893dnZkWU6lUi8T9wpVq1Vd11OpVEtLi8/nw3EcAMCyrN/vR1H0/v6+UCicnJzs7e0lEglFUarVqmmaL9tfpx8A4HQ6BwYG+vv7WZYlSdI0TcMwrFYrhmH7+/tzc3Pb29vZbPb5+fn35/7lG4EQ1tbW1tXVMQzT0NBgGAaEsKOjo62tLZvNSpJ0dHT0rvIW+rGEohaLBcMw0zRRFKUoiiCIp6cnXdeLxeK7yv/6VH0DZUU3/XCrlnYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=24x24 at 0x7EFE0F2AFD90>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAIAAABvFaqvAAAB/0lEQVR4nN2TSwspYRjHzZjJZXLrNUq5RtmaSY1LrKaMhYXLytLC3nfwQawsbRWhNKSMkpRySTIlQhYUIzNnczp16kzHWJ3Of/v+nt/7PE/vq1L9a4EU0S6XKxwOBwKB6XTKsuz5fP51hHyogGHY4XCkUqlcLmexWB6PB8dxvwEfigAAyWQyk8ngOM6y7GQyud1uH9b+DARBGo0mm812u93hcFgsFrVaLQQp24lKpVJpNBqGYXq93mazqVQqOp3uG4ter6coqtVqXa/Xdrudz+flyL/sCABA0zRJksfjsdVqjcfjb0RmszkYDKbTaYPBMJ/PR6PRbrf7RuRyuWiapigKQRBJkgAAAAA5WPYdIQjidrtJkpQk6f1+Mwxjt9tNJlO1Wn29Xgo6cjqdsVgsFArd7/f1en04HAKBQCqVIghC2Wher9fv9z+fz36/Xy6Xa7Xafr/HMAzDMGUin8/n9/sfj8d2uz2dTgAAFEV5nl+tVspENpsNx3FRFNVqdaFQiMfjoiguFgue5//Iyy4bRVEURWEYdrvd0WgUQZBmszmbzSRJUtaRIAiCIBiNRoIgYBje7/eNRmMwGMjxsqFpul6vv16v0+nU6XRKpZLH44Fh2Ytlv5/Vak0kEpFIZLlcchzH8/z1ehUEQbFIrVYDAKxW6+VyOR6Poih+Ps1/lh/nJcXUk5MBywAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "winSize = desired_dimension\n",
        "blockSize = (16,16)\n",
        "blockStride = (8,8)\n",
        "cellSize = (8,8)\n",
        "nbins = 9\n",
        "derivAperture = 1\n",
        "winSigma = 4.\n",
        "histogramNormType = 0\n",
        "L2HysThreshold = 2.0000000000000001e-01\n",
        "gammaCorrection = 0\n",
        "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,\n",
        "                        histogramNormType,L2HysThreshold,gammaCorrection)\n",
        "\n",
        "winStride = (8,8)\n",
        "padding = (8,8)\n",
        "locations = ((0,0),)\n",
        "\n",
        "train_cut_hog_feature = np.zeros((len(train_cut), 4*4*nbins))\n",
        "for i in range(len(train_cut)):\n",
        "    feature_vector = hog.compute(train_cut[i],winStride,padding,locations)\n",
        "    feature_vector = feature_vector.reshape(-1)\n",
        "    train_cut_hog_feature[i] = feature_vector\n",
        "\n",
        "\n",
        "test_cut_hog_feature = np.zeros((len(test_cut), 4*4*nbins))\n",
        "for i in range(len(test_cut)):\n",
        "    feature_vector = hog.compute(test_cut[i],winStride,padding,locations)\n",
        "    feature_vector = feature_vector.reshape(-1) # Flatten the 3D array to 1D array\n",
        "    test_cut_hog_feature[i] = feature_vector\n",
        "\n",
        "print(train_cut_hog_feature.shape, len(train_cut_label))\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "knn_classifier.fit(train_cut_hog_feature, train_cut_label)\n",
        "HOG_K1 = knn_classifier.predict(test_cut_hog_feature)\n",
        "\n",
        "accuracy = sum(HOG_K1 == test_cut_label) / len(test_cut_label)\n",
        "print(\"HOG_K1 Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdeprkf3SoIf",
        "outputId": "8fbc1833-1f0f-4287-e5e7-aa79aff4cc21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12, 144) 12\n",
            "HOG_K1 Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"\"\n",
        "fileName51 = path + \"5.1.png\" \n",
        "fileName52 = path + \"5.2.png\" \n",
        "fileName53 = path + \"5.3.png\" \n",
        "fileName54 = path + \"5.4.png\" \n",
        "\n",
        "#RGB images in BGR order in OpenCV\n",
        "image51 = cv2.imread(fileName51, cv2.IMREAD_COLOR)\n",
        "image51 = cv2.resize(image51, desired_dimension)\n",
        "image52 = cv2.imread(fileName52, cv2.IMREAD_COLOR)\n",
        "image52 = cv2.resize(image52, desired_dimension)\n",
        "image53 = cv2.imread(fileName53, cv2.IMREAD_COLOR)\n",
        "image53 = cv2.resize(image53, desired_dimension)\n",
        "image54 = cv2.imread(fileName54, cv2.IMREAD_COLOR)\n",
        "image54 = cv2.resize(image54, desired_dimension)\n",
        "\n",
        "desired_dimension = (24, 24)\n",
        "test_cut_label = [5, 5, 5, 5]\n",
        "winSize = desired_dimension\n",
        "blockSize = (16,16)\n",
        "blockStride = (8,8)\n",
        "cellSize = (8,8)\n",
        "nbins = 9\n",
        "derivAperture = 1\n",
        "winSigma = 4.\n",
        "histogramNormType = 0\n",
        "L2HysThreshold = 2.0000000000000001e-01\n",
        "gammaCorrection = 0\n",
        "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,\n",
        "                        histogramNormType,L2HysThreshold,gammaCorrection)\n",
        "\n",
        "winStride = (8,8)\n",
        "padding = (8,8)\n",
        "locations = ((0,0),)\n",
        "\n",
        "my_imshow(\"imh51\", image51)\n",
        "my_imshow(\"imh52\", image52)\n",
        "my_imshow(\"imh53\", image53)\n",
        "my_imshow(\"imh54\", image54)\n",
        "\n",
        "train_cut_hog_feature = np.zeros((len(train_cut), 4*4*nbins))\n",
        "for i in range(len(train_cut)):\n",
        "    feature_vector = hog.compute(train_cut[i],winStride,padding,locations)\n",
        "    feature_vector = feature_vector.reshape(-1)\n",
        "    train_cut_hog_feature[i] = feature_vector\n",
        "\n",
        "test_cut = [image51, image52, image53, image54]\n",
        "test_cut_hog_feature = np.zeros((len(test_cut), 4*4*nbins))\n",
        "for i in range(len(test_cut)):\n",
        "    feature_vector = hog.compute(test_cut[i],winStride,padding,locations)\n",
        "    feature_vector = feature_vector.reshape(-1) # Flatten the 3D array to 1D array\n",
        "    test_cut_hog_feature[i] = feature_vector\n",
        "\n",
        "print(train_cut_hog_feature.shape, len(train_cut_label))\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "knn_classifier.fit(train_cut_hog_feature, train_cut_label)\n",
        "HOG_K1 = knn_classifier.predict(test_cut_hog_feature)\n",
        "\n",
        "accuracy = sum(HOG_K1 == test_cut_label) / len(test_cut_label)\n",
        "print(\"HOG_K1 Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "-ByC-9APXAUo",
        "outputId": "95eba7c5-fc2b-4821-db30-470e586785ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imh51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=24x24 at 0x7EFE0F2502B0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAIAAABvFaqvAAAAv0lEQVR4nO2SMRKEIAxFE1kaC07JFai8h/exsfUIDIUHsLBhgGxhsy4MKIPFzmwqJiSP5PORiKBFdE0ovwIax7EChLHYiJjvCSHENTWrbduWyFIUwzAk8/lINCzLUgFKaHTIdNeozb7/lbnb9x0AELHv+xrQuq5wNgFjzDmXByVWE0IcB6WU1lpK6b0vTlT+HWPMlbJTBQCEED4z8zxffO9bo65LLGutLW52aou9M00TEXHOi6Dbxrs00R/0LOgNDsU8yYIcth4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imh52\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=24x24 at 0x7EFE0F250370>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAIAAABvFaqvAAAA80lEQVR4nN2UrQ6DMBDH75aRVEBCKvswWDQJT4DCIbAIXgbHk+BB8AoERSW9TpBsDFo2lollf9Xe9X69j6aotYZv6PIVyn+DrntTWZbTNHHOjQFt29Z1jYjjOLqu+3DoZ72fQpZl60DcBM/zzBhTSkVRFASBEZGmKWNsa9U7NU0DAJzzvetA24wAQErped5yx/uVGkAAgIhnQUfjJ6I4jnGlYRisp40F2w77vm/r0QtQGIZd162N50BE1Pe9lHLZVlV1f5/nQEmSCCGEEJvS8jw/B9p3pygKIrJRtPEdLfNSSi1rx3Gsk1rJDPpAv/cf/THoBsDynw8/v4Z3AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imh53\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=24x24 at 0x7EFE0F250520>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAIAAABvFaqvAAAAr0lEQVR4nO2SMRKEIAxFE4YjWNBaWXMYaippuZJX8gSh4BQUsM3uDhJ1ccZufR0DefkBsJQCdyBusTyiN8uyTNOEFZvtUpFzXtcVAIiIiJxzwzBw4ziOhbERHWWZ5znGyIsPRV+d9/68jIM8iBDiPOAuO5ctpQSAEIIxBhGVUl0mHjLGeJTx2mgcxN/H2tG01l2D8GZNq/abfbicKKVUL621RNQ1/tVn7k30iP5K9AJRkvw5fWzb0QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imh54\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=24x24 at 0x7EFE0F250340>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAIAAABvFaqvAAAAxElEQVR4nO2UsQ2EIBSGfyyMjUMYhsCGWaxdwEmonAI3MHEdK6xo4ApzxtN3iMZcYnJfBeTxPfgTYN573EFyi+UporZtWRBSxPZhW2uzLJNS0p2TpO/7KBEAxuj1AD8J2xhjjIk1eYpNzTiOZNnHloBIKSWEmMfOuSuiqqqW8TRNAOq6viLaH3CtJtmGzTlvmiY24EDYANI0XaZa67lsGIZzVyuKAkCe5+tmSqnj6xNLb8qy7LruUDFz+il84xH/0V+04QV/f7rYLZqIBAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12, 144) 12\n",
            "HOG_K1 Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('digits.png', 0)\n",
        "\n",
        "thresh = cv2.threshold(img, 0, 255, cv2.THRESH_OTSU)[1]\n",
        "\n",
        "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(thresh, connectivity=8)\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "for i in range(1, num_labels):\n",
        "    left, top, width, height, _ = stats[i]\n",
        "\n",
        "    digit_img = img[top:top + height, left:left + width]\n",
        "\n",
        "    resized_img = cv2.resize(digit_img, (24, 24))\n",
        "\n",
        "    images.append(resized_img)\n",
        "    labels.append(i // 500)\n",
        "\n",
        "img_train, img_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "desired_dimension = (24, 24)\n",
        "test_cut_label = [5, 5, 5, 5]\n",
        "winSize = desired_dimension\n",
        "blockSize = (16,16)\n",
        "blockStride = (8,8)\n",
        "cellSize = (8,8)\n",
        "nbins = 9\n",
        "derivAperture = 1\n",
        "winSigma = 4.\n",
        "histogramNormType = 0\n",
        "L2HysThreshold = 2.0000000000000001e-01\n",
        "gammaCorrection = 0\n",
        "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,\n",
        "                        histogramNormType,L2HysThreshold,gammaCorrection)\n",
        "\n",
        "train_features = np.array([hog.compute(cv2.resize(img, desired_dimension)) for img in img_train]).reshape(len(img_train), -1)\n",
        "\n",
        "test_features = np.array([hog.compute(cv2.resize(img, desired_dimension)) for img in img_test]).reshape(len(img_test), -1)\n",
        "\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "knn_classifier.fit(train_features, y_train)\n",
        "\n",
        "predictions = knn_classifier.predict(test_features)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "\n",
        "print(\"HOG_K1 Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R61qLOIVg3T_",
        "outputId": "b095d521-76de-4f90-a560-90c9f597b8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOG_K1 Accuracy: 0.8204121687929342\n"
          ]
        }
      ]
    }
  ]
}